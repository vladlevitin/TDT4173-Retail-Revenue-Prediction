{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import *\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sqlalchemy import column\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#!pip install optuna \n",
    "import optuna\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_category(X):\n",
    "    for col in X.columns:\n",
    "        if ( (X[col].dtype != 'int64') and (X[col].dtype != 'float64') and (X[col].dtype != 'bool')):\n",
    "            X[col] = X[col].astype('category')\n",
    "            \n",
    "    return X\n",
    "\n",
    "def remove_outliers(df, column_names, n):\n",
    "    for column_name in column_names:\n",
    "        mean = df[column_name].mean()\n",
    "        std = df[column_name].std()\n",
    "        df = df[(df[column_name] > mean - n*std) & (df[column_name] < mean + n*std)]\n",
    "    return df\n",
    "    \n",
    "def log_func(y, shift_var):\n",
    "    return np.log(y+shift_var)\n",
    "\n",
    "def exp_func(y, shift_var):\n",
    "    return np.exp(y)-shift_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Root Mean Squared Logarithmic Error \n",
    "    \n",
    "    Args:\n",
    "        y_true (np.array): n-dimensional vector of ground-truth values \n",
    "        y_pred (np.array): n-dimensional vecotr of predicted values \n",
    "    \n",
    "    Returns:\n",
    "        A scalar float with the rmsle value \n",
    "    \n",
    "    Note: You can alternatively use sklearn and just do: \n",
    "        `sklearn.metrics.mean_squared_log_error(y_true, y_pred) ** 0.5`\n",
    "    \"\"\"\n",
    "    assert (y_true >= 0).all(), 'Received negative y_true values'\n",
    "    assert (y_pred >= 0).all(), 'Received negative y_pred values'\n",
    "    assert y_true.shape == y_pred.shape, 'y_true and y_pred have different shapes'\n",
    "    y_true_log1p = np.log1p(y_true)  # log(1 + y_true)\n",
    "    y_pred_log1p = np.log1p(y_pred)  # log(1 + y_pred)\n",
    "    return np.sqrt(np.mean(np.square(y_pred_log1p - y_true_log1p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = pd.read_csv('data/stores_train_with_extra_features.csv')\n",
    "stores_test = pd.read_csv('data/stores_test_with_extra_features.csv')\n",
    "\n",
    "plaace_hierarchy = pd.read_csv('data/plaace_hierarchy.csv')\n",
    "grunnkrets = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "grunnkrets_ages = pd.read_csv('data/grunnkrets_age_distribution.csv')\n",
    "grunnkrets_household_types = pd.read_csv('data/grunnkrets_households_num_persons.csv')\n",
    "grunnkrets_household_income = pd.read_csv('data/grunnkrets_income_households.csv')\n",
    "\n",
    "stores_train_copy= stores_train.copy()\n",
    "\n",
    "plaace_hierarchy_copy = plaace_hierarchy.copy()\n",
    "plaace_hierarchy_copy.drop(columns='sales_channel_name', inplace=True)\n",
    "plaace_hierarchy_copy['lv1'] = plaace_hierarchy_copy['lv1'].astype('category')\n",
    "plaace_hierarchy_copy['lv2'] = plaace_hierarchy_copy['lv2'].astype('category')\n",
    "\n",
    "\n",
    "grunnkrets_copy = grunnkrets.copy()\n",
    "grunnkrets_copy.rename(columns={'year': 'year_1'}, inplace=True)\n",
    "\n",
    "grunnkrets_ages_copy = grunnkrets_ages.copy()\n",
    "grunnkrets_ages_copy.rename(columns={'year': 'year_2'}, inplace=True)\n",
    "\n",
    "grunnkrets_household_types_copy = grunnkrets_household_types.copy()\n",
    "grunnkrets_household_types_copy.rename(columns={'year': 'year_3'}, inplace=True)\n",
    "grunnkrets_household_types_copy[grunnkrets_household_types_copy.columns[2:]] = grunnkrets_household_types_copy[grunnkrets_household_types_copy.columns[2:]].astype('int64')\n",
    "\n",
    "grunnkrets_household_income_copy = grunnkrets_household_income.copy()\n",
    "grunnkrets_household_income_copy.rename(columns={'year': 'year_4', 'singles': 'singles_income','couple_without_children':'couple_without_children_income'}, inplace=True)\n",
    "grunnkrets_household_income_copy.rename(columns={'singles': 'singles_income', }, inplace=True)\n",
    "\n",
    "#set the values that are 0 to the lowest value in the column\n",
    "# for column in grunnkrets_household_income_copy.columns[2:]:\n",
    "#     grunnkrets_household_income_copy[column] = grunnkrets_household_income_copy[column].apply(lambda x: grunnkrets_household_income_copy[column].min() if x == 0 else x)\n",
    "    \n",
    "\n",
    "df = stores_train_copy\n",
    "\n",
    "df = pd.merge(df, plaace_hierarchy_copy, on='plaace_hierarchy_id', how='left')\n",
    "df = pd.merge(df, grunnkrets_copy, on='grunnkrets_id', how='left')\n",
    "df = pd.merge(df, grunnkrets_ages_copy, on='grunnkrets_id', how='left')\n",
    "df = pd.merge(df, grunnkrets_household_types_copy, on='grunnkrets_id', how='left')\n",
    "df = pd.merge(df, grunnkrets_household_income_copy, on='grunnkrets_id', how='left')\n",
    "\n",
    "df.drop_duplicates(subset=['store_id'], keep='first', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "columns_to_drop = ['revenue',\n",
    "                  'store_id',\n",
    "                  'plaace_hierarchy_id',\n",
    "                  'grunnkrets_id',\n",
    "                  'year',\n",
    "                   'address',\n",
    "                  'store_name',\n",
    "                  \n",
    "                  'year_1',\n",
    "                  'geometry',\n",
    "                  \n",
    "                  #'area_km2',\n",
    "               \n",
    "                  'grunnkrets_name',\n",
    "                  'district_name',\n",
    "             \n",
    "                  'municipality_name',\n",
    "                  \n",
    "                  \n",
    "                  'year_2',\n",
    "                  'year_3',\n",
    "                  'year_4',\n",
    "                 \n",
    "                \n",
    "                  'sales_channel_name',\n",
    "                   #'mall_name',\n",
    "                  #'chain_name',\n",
    "                  \n",
    "                  \n",
    "                  \n",
    "                     'lv1',\n",
    "                  'lv2',\n",
    "                  'lv3',\n",
    "                  'lv4',\n",
    "                  # 'lv1_desc',\n",
    "                  # 'lv2_desc',\n",
    "                  # 'lv3_desc',\n",
    "                  # 'lv4_desc'\n",
    "                \n",
    "   #              'closest_busstop',\n",
    "                  \n",
    "   #  'num_closest_busstops_250m',\n",
    "   # \n",
    "   # 'num_closest_busstops_500m',\n",
    "\n",
    "   #     'num_closest_busstops_1000m', 'num_closest_busstops_2500m',\n",
    "   #     'num_closest_busstops_5000m', 'num_closest_busstops',\n",
    "   #     'num_closest_busstops_10000m', 'num_closest_busstops_15000m',\n",
    "       \n",
    "   #     'closest_store_lv1', 'closest_store_lv2',\n",
    "   #     'closest_store_lv3', 'closest_store_lv4', \n",
    "       \n",
    "   #     'num_closest_stores_lv1_250m',   'num_closest_stores_lv1_500m', \n",
    "   #  'num_closest_stores_lv1_1000m',\n",
    "    \n",
    "   #  'num_closest_stores_lv1_2500m',\n",
    "   #     'num_closest_stores_lv1_5000m', 'num_closest_stores_lv1',\n",
    "   #     'num_closest_stores_lv1_10000m', 'num_closest_stores_lv1_15000m',\n",
    "       \n",
    "   #     'num_closest_stores_lv2_250m', 'num_closest_stores_lv2_500m',\n",
    "   #  'num_closest_stores_lv2_1000m', \n",
    "    \n",
    "   #  'num_closest_stores_lv2_2500m',\n",
    "   #     'num_closest_stores_lv2_5000m', 'num_closest_stores_lv2',\n",
    "   #     'num_closest_stores_lv2_10000m', 'num_closest_stores_lv2_15000m',\n",
    "       \n",
    "   #      'num_closest_stores_lv3_250m', 'num_closest_stores_lv3_500m',\n",
    "   #     'num_closest_stores_lv3_1000m', 'num_closest_stores_lv3_2500m',\n",
    "   #     'num_closest_stores_lv3_5000m', 'num_closest_stores_lv3',\n",
    "   #     'num_closest_stores_lv3_10000m', 'num_closest_stores_lv3_15000m',\n",
    "       \n",
    "   #      'num_closest_stores_lv4_250m', 'num_closest_stores_lv4_500m',\n",
    "   #     'num_closest_stores_lv4_1000m', 'num_closest_stores_lv4_2500m',\n",
    "   #     'num_closest_stores_lv4_5000m', 'num_closest_stores_lv4',\n",
    "   #     'num_closest_stores_lv4_10000m', 'num_closest_stores_lv4_15000m',\n",
    "                  #'distance_to_oslo',\n",
    "                  'distance_to_bergen',\n",
    "                  'distance_to_trondheim',\n",
    "                  'distance_to_stavanger',\n",
    "                  'distance_to_drammen',\n",
    "                  \n",
    "                  ]\n",
    "\n",
    "\n",
    "# fill in missing values of float columns with mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = df.drop(columns=columns_to_drop)\n",
    "X = convert_to_category(X)\n",
    "\n",
    "# select columns that are of type int or float\n",
    "\n",
    "\n",
    "\n",
    "categorical_features = X.select_dtypes(include=['category']).columns\n",
    "numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "        \n",
    "full_pipeline = ColumnTransformer([('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)], remainder='passthrough')\n",
    "one_hot_encoder = full_pipeline.fit(X)\n",
    "X_encoded_one_hot = one_hot_encoder.transform(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y = df.revenue\n",
    "y = np.log1p(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.integration import LightGBMPruningCallback\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "random_state = 10\n",
    "test_size = 0.3\n",
    "\n",
    "def objective(trial,data=X,target=y):\n",
    "\n",
    "    params = {\n",
    "        'metric': 'rmse', \n",
    "        'random_state': 48,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
    "      \n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
    "        #'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
    "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.3, 1),\n",
    "        #'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n",
    "        'subsample': trial.suggest_loguniform('subsample', 0.4, 1),\n",
    "        #'learning_rate': trial.suggest_categorical('learning_rate', [0.006,0.008,0.01,0.014,0.017,0.02, 0.03, 0.04, 0.05]),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.006, 0.05),\n",
    "        #'max_depth': trial.suggest_categorical('max_depth', [10,20,100]),\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 100),\n",
    "        \n",
    "        'num_leaves' : trial.suggest_int('num_leaves', 10, 1000),\n",
    "        \n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n",
    "    }\n",
    "    \n",
    "    # model = lgbm.LGBMRegressor(**params)\n",
    "    \n",
    "    # train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # model.fit(train_x,train_y, eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n",
    "    \n",
    "    # preds = model.predict(test_x)\n",
    "    \n",
    "    # test_y_exp = np.expm1(test_y)\n",
    "    # preds_exp = np.expm1(preds)\n",
    "    \n",
    "    # preds_exp = np.where(preds_exp < 0, 0, preds_exp)\n",
    "    \n",
    "    # error = rmsle( test_y_exp, preds_exp)\n",
    "    \n",
    "    # return error\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    kf_scores = np.empty(5)\n",
    "    \n",
    "    model = lgbm.LGBMRegressor(**params)\n",
    "    \n",
    "    \n",
    "    for idx, (train_index, test_index) in enumerate(kf.split(X_encoded_label, y)):\n",
    "        X_train, X_test = X_encoded_label.iloc[train_index], X_encoded_label.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "    \n",
    "        model.fit(X_train,y_train, eval_set=[(X_test,y_test)],early_stopping_rounds=100,verbose=False)\n",
    "        \n",
    "        preds = model.predict(X_test)\n",
    "        \n",
    "        y_test_exp = np.expm1(y_test)\n",
    "        preds_exp = np.expm1(preds)\n",
    "        \n",
    "        preds_exp = np.where(preds_exp < 0, 0, preds_exp)\n",
    "        \n",
    "        kf_scores[idx] = rmsle(y_test_exp, preds_exp)\n",
    "\n",
    "    return np.mean(kf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-08 04:57:44,178]\u001b[0m A new study created in memory with name: no-name-d7db1adf-245e-4777-8dd4-b22f28fbfffa\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-08 04:58:41,586]\u001b[0m Trial 0 finished with value: 0.7598379916255997 and parameters: {'n_estimators': 2222, 'reg_alpha': 0.17478694042136667, 'reg_lambda': 0.0021444922560086006, 'colsample_bytree': 0.5929673766609653, 'subsample': 0.44056214488685924, 'learning_rate': 0.020467480493871752, 'max_depth': 14, 'num_leaves': 918, 'min_child_samples': 3}. Best is trial 0 with value: 0.7598379916255997.\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-08 04:58:46,211]\u001b[0m Trial 1 finished with value: 0.7426882887426511 and parameters: {'n_estimators': 855, 'reg_alpha': 0.31282678716841605, 'reg_lambda': 0.020598357103018054, 'colsample_bytree': 0.9634146618579748, 'subsample': 0.872936965520032, 'learning_rate': 0.021821363133166356, 'max_depth': 70, 'num_leaves': 400, 'min_child_samples': 273}. Best is trial 1 with value: 0.7426882887426511.\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-08 04:59:00,533]\u001b[0m Trial 2 finished with value: 0.7423925354168924 and parameters: {'n_estimators': 1346, 'reg_alpha': 0.03631440506979739, 'reg_lambda': 0.029835853115636947, 'colsample_bytree': 0.36446913155189303, 'subsample': 0.48355391179463797, 'learning_rate': 0.00625678983853434, 'max_depth': 93, 'num_leaves': 118, 'min_child_samples': 89}. Best is trial 2 with value: 0.7423925354168924.\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-08 04:59:03,908]\u001b[0m Trial 3 finished with value: 0.7406544236527515 and parameters: {'n_estimators': 961, 'reg_alpha': 0.12991599628213624, 'reg_lambda': 0.007044703817975568, 'colsample_bytree': 0.521197813643087, 'subsample': 0.6024881156258173, 'learning_rate': 0.031141388654562455, 'max_depth': 97, 'num_leaves': 36, 'min_child_samples': 185}. Best is trial 3 with value: 0.7406544236527515.\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-08 04:59:10,249]\u001b[0m Trial 4 finished with value: 0.749193589433266 and parameters: {'n_estimators': 591, 'reg_alpha': 0.0012296543743494331, 'reg_lambda': 0.3679813947726713, 'colsample_bytree': 0.31681302791409816, 'subsample': 0.4346645491822003, 'learning_rate': 0.007541206991287981, 'max_depth': 72, 'num_leaves': 982, 'min_child_samples': 147}. Best is trial 3 with value: 0.7406544236527515.\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-08 04:59:13,615]\u001b[0m Trial 5 finished with value: 0.7416429610202228 and parameters: {'n_estimators': 1861, 'reg_alpha': 2.6620765933112285, 'reg_lambda': 1.8157704914116386, 'colsample_bytree': 0.7663875773225007, 'subsample': 0.8318356481563494, 'learning_rate': 0.04452962892925511, 'max_depth': 23, 'num_leaves': 900, 'min_child_samples': 229}. Best is trial 3 with value: 0.7406544236527515.\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-08 04:59:19,216]\u001b[0m Trial 6 finished with value: 0.7426946672586237 and parameters: {'n_estimators': 1496, 'reg_alpha': 1.405472132608492, 'reg_lambda': 6.027178246793815, 'colsample_bytree': 0.5454577488002894, 'subsample': 0.5013836877105562, 'learning_rate': 0.016642236348909063, 'max_depth': 45, 'num_leaves': 946, 'min_child_samples': 240}. Best is trial 3 with value: 0.7406544236527515.\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-08 04:59:22,561]\u001b[0m Trial 7 finished with value: 0.7451022577404596 and parameters: {'n_estimators': 1013, 'reg_alpha': 0.0019801653550454226, 'reg_lambda': 0.001006818203426216, 'colsample_bytree': 0.39966498367259984, 'subsample': 0.7879469776832879, 'learning_rate': 0.03277091542478483, 'max_depth': 47, 'num_leaves': 327, 'min_child_samples': 296}. Best is trial 3 with value: 0.7406544236527515.\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-08 04:59:36,793]\u001b[0m Trial 8 finished with value: 0.773976678511146 and parameters: {'n_estimators': 958, 'reg_alpha': 0.23243568634766798, 'reg_lambda': 0.0896050393225162, 'colsample_bytree': 0.3342391226541073, 'subsample': 0.5704682745846724, 'learning_rate': 0.04672488212786565, 'max_depth': 66, 'num_leaves': 462, 'min_child_samples': 11}. Best is trial 3 with value: 0.7406544236527515.\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-08 04:59:39,036]\u001b[0m Trial 9 finished with value: 0.751592822679058 and parameters: {'n_estimators': 115, 'reg_alpha': 0.8337804234426643, 'reg_lambda': 0.1217781610377833, 'colsample_bytree': 0.8545313715342504, 'subsample': 0.9744263882356579, 'learning_rate': 0.019947288142737687, 'max_depth': 17, 'num_leaves': 248, 'min_child_samples': 189}. Best is trial 3 with value: 0.7406544236527515.\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-08 04:59:49,984]\u001b[0m Trial 10 finished with value: 0.7397803485668735 and parameters: {'n_estimators': 2923, 'reg_alpha': 0.02052584033166758, 'reg_lambda': 0.0050318252265558715, 'colsample_bytree': 0.5012617312697185, 'subsample': 0.6799489094848151, 'learning_rate': 0.010833275205385315, 'max_depth': 99, 'num_leaves': 656, 'min_child_samples': 128}. Best is trial 10 with value: 0.7397803485668735.\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-08 04:59:58,767]\u001b[0m Trial 11 finished with value: 0.7392899955967815 and parameters: {'n_estimators': 2843, 'reg_alpha': 0.016407865924930227, 'reg_lambda': 0.0057351915314327135, 'colsample_bytree': 0.5292478773047525, 'subsample': 0.6802219999326548, 'learning_rate': 0.011529043668361107, 'max_depth': 97, 'num_leaves': 624, 'min_child_samples': 124}. Best is trial 11 with value: 0.7392899955967815.\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-08 05:00:08,210]\u001b[0m Trial 12 finished with value: 0.7404368076791865 and parameters: {'n_estimators': 2997, 'reg_alpha': 0.014801733409395258, 'reg_lambda': 0.00579843647218402, 'colsample_bytree': 0.45045777676678206, 'subsample': 0.7047475797067541, 'learning_rate': 0.011308078192271198, 'max_depth': 85, 'num_leaves': 658, 'min_child_samples': 88}. Best is trial 11 with value: 0.7392899955967815.\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-08 05:00:17,342]\u001b[0m Trial 13 finished with value: 0.7373231360861339 and parameters: {'n_estimators': 2908, 'reg_alpha': 0.00852256263020135, 'reg_lambda': 0.005191261962830287, 'colsample_bytree': 0.6600627881308588, 'subsample': 0.6575896899481727, 'learning_rate': 0.011039757887270631, 'max_depth': 100, 'num_leaves': 644, 'min_child_samples': 106}. Best is trial 13 with value: 0.7373231360861339.\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-08 05:00:28,261]\u001b[0m Trial 14 finished with value: 0.7374069165949406 and parameters: {'n_estimators': 2560, 'reg_alpha': 0.006271249017980633, 'reg_lambda': 0.03566177729104017, 'colsample_bytree': 0.7163329672676532, 'subsample': 0.7120060039719309, 'learning_rate': 0.01208731741415595, 'max_depth': 80, 'num_leaves': 644, 'min_child_samples': 71}. Best is trial 13 with value: 0.7373231360861339.\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-08 05:00:38,578]\u001b[0m Trial 15 finished with value: 0.7372180765380107 and parameters: {'n_estimators': 2372, 'reg_alpha': 0.004493606640275611, 'reg_lambda': 0.02755042530277799, 'colsample_bytree': 0.6637554149820615, 'subsample': 0.5710318824163434, 'learning_rate': 0.014695695894221424, 'max_depth': 82, 'num_leaves': 767, 'min_child_samples': 58}. Best is trial 15 with value: 0.7372180765380107.\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-08 05:00:51,878]\u001b[0m Trial 16 finished with value: 0.7370983726743868 and parameters: {'n_estimators': 2199, 'reg_alpha': 0.004581541389895453, 'reg_lambda': 0.17669488488716203, 'colsample_bytree': 0.6463389545170887, 'subsample': 0.5599501706732359, 'learning_rate': 0.015066207310602467, 'max_depth': 58, 'num_leaves': 781, 'min_child_samples': 53}. Best is trial 16 with value: 0.7370983726743868.\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-08 05:01:04,946]\u001b[0m Trial 17 finished with value: 0.7370818575630856 and parameters: {'n_estimators': 2128, 'reg_alpha': 0.0032850593468303447, 'reg_lambda': 0.8646923137299712, 'colsample_bytree': 0.6427913512543559, 'subsample': 0.5405566133534582, 'learning_rate': 0.01550426082979914, 'max_depth': 54, 'num_leaves': 793, 'min_child_samples': 51}. Best is trial 17 with value: 0.7370818575630856.\u001b[0m\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reg_alpha': 0.06274878529229336,\n",
       " 'reg_lambda': 5.186996998261939,\n",
       " 'colsample_bytree': 0.7193075915560254,\n",
       " 'subsample': 0.761884354938652,\n",
       " 'learning_rate': 0.047109971283356256,\n",
       " 'max_depth': 84,\n",
       " 'num_leaves': 51,\n",
       " 'min_child_samples': 2}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = study.best_trial.params\n",
    "val = study.best_trial.value\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# params = study.best_trial.params\n",
    "# val = study.best_trial.value\n",
    "\n",
    "# dict = {'val': val, 'params': params}\n",
    "\n",
    "# # save the params and value\n",
    "\n",
    "# with open('lgbm_params.json', 'w') as fp:\n",
    "#     json.dump(dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 12\n",
    "\n",
    "def objective(trial,data=X_encoded_one_hot,target=y):\n",
    " \n",
    "    # param = {}\n",
    "    # param['learning_rate'] = trial.suggest_discrete_uniform(\"learning_rate\", 0.001, 0.05, 0.001)\n",
    "    # param['depth'] = trial.suggest_int('depth', 2, 10)\n",
    "    # param['l2_leaf_reg'] = trial.suggest_discrete_uniform('l2_leaf_reg', 1.0, 5.5, 0.5)\n",
    "    # param['min_child_samples'] = trial.suggest_categorical('min_child_samples', [1, 4, 8, 16, 32])\n",
    "    # param['grow_policy'] = 'Depthwise'\n",
    "    # #param['iterations'] = 10000\n",
    "    # param['use_best_model'] = True\n",
    "    # param['eval_metric'] = 'RMSE'\n",
    "    # param['od_type'] = 'iter'\n",
    "    # param['od_wait'] = 20\n",
    "    # param['random_state'] = 42\n",
    "    # param['logging_level'] = 'Silent'\n",
    "    # param['task_type'] = 'GPU'\n",
    "    \n",
    "    params = {\n",
    "        'iterations':trial.suggest_int(\"iterations\", 4000, 25000),\n",
    "        'od_wait':trial.suggest_int('od_wait', 500, 2300),\n",
    "        'learning_rate' : trial.suggest_uniform('learning_rate',0.01, 1),\n",
    "        'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n",
    "        'subsample': trial.suggest_uniform('subsample',0,1),\n",
    "        'random_strength': trial.suggest_uniform('random_strength',10,50),\n",
    "        'depth': trial.suggest_int('depth',1, 9),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n",
    "        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n",
    "        'loss_function':\"RMSE\",\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = cat.CatBoostRegressor(**params)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    kf_scores = np.empty(5)\n",
    "    \n",
    "    X_selected = X_encoded_one_hot\n",
    "    \n",
    "    for idx, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
    "        X_train, X_test = X_selected[train_index], X_selected[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "  \n",
    "        model.fit(X_train, y_train,eval_set=[(X_test,y_test)], early_stopping_rounds=100, verbose=False)\n",
    "    \n",
    "        preds = model.predict(X_test)\n",
    "        \n",
    "        y_test_exp = np.expm1(y_test)\n",
    "        preds_exp = np.expm1(preds)\n",
    "        \n",
    "        preds_exp = np.where(preds_exp < 0, 0, preds_exp)\n",
    "        \n",
    "        kf_scores[idx] = rmsle(y_test_exp, preds_exp)\n",
    "\n",
    "    return np.mean(kf_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-07 23:13:21,457]\u001b[0m A new study created in memory with name: no-name-bdd5997f-5639-40e2-af5a-e4b48757b607\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 23:13:39,278]\u001b[0m Trial 0 finished with value: 0.737189923856096 and parameters: {'iterations': 23718, 'od_wait': 1282, 'learning_rate': 0.2723694482083586, 'reg_lambda': 63.08150055782134, 'subsample': 0.516008154573019, 'random_strength': 39.75911175074713, 'depth': 7, 'min_data_in_leaf': 24, 'leaf_estimation_iterations': 12}. Best is trial 0 with value: 0.737189923856096.\u001b[0m\n",
      "\u001b[32m[I 2022-11-07 23:14:44,429]\u001b[0m Trial 1 finished with value: 0.7828460699557535 and parameters: {'iterations': 8057, 'od_wait': 769, 'learning_rate': 0.5754175032930271, 'reg_lambda': 43.807470916712006, 'subsample': 0.16428061290605855, 'random_strength': 40.74272574169472, 'depth': 10, 'min_data_in_leaf': 2, 'leaf_estimation_iterations': 14}. Best is trial 0 with value: 0.737189923856096.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNumber of finished trials:\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(study\u001b[39m.\u001b[39mtrials))\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest trial:\u001b[39m\u001b[39m'\u001b[39m, study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\optuna\\study\\study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[39mif\u001b[39;00m n_jobs \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    393\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    394\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    395\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis feature will be removed in v4.0.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    396\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    397\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m     )\n\u001b[1;32m--> 400\u001b[0m _optimize(\n\u001b[0;32m    401\u001b[0m     study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    402\u001b[0m     func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    403\u001b[0m     n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    404\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    405\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    406\u001b[0m     catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[0;32m    407\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    408\u001b[0m     gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    409\u001b[0m     show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    410\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m show_progress_bar:\n",
      "File \u001b[1;32mc:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\optuna\\study\\_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    210\u001b[0m     thread\u001b[39m.\u001b[39mstart()\n\u001b[0;32m    212\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 213\u001b[0m     value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn [37], line 47\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial, data, target)\u001b[0m\n\u001b[0;32m     44\u001b[0m X_train, X_test \u001b[39m=\u001b[39m X_selected[train_index], X_selected[test_index]\n\u001b[0;32m     45\u001b[0m y_train, y_test \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39miloc[train_index], y\u001b[39m.\u001b[39miloc[test_index]\n\u001b[1;32m---> 47\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train,eval_set\u001b[39m=\u001b[39;49m[(X_test,y_test)], early_stopping_rounds\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     49\u001b[0m preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     51\u001b[0m y_test_exp \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpm1(y_test)\n",
      "File \u001b[1;32mc:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\catboost\\core.py:5730\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5727\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[0;32m   5728\u001b[0m     CatBoostRegressor\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m-> 5730\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline,\n\u001b[0;32m   5731\u001b[0m                  use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description,\n\u001b[0;32m   5732\u001b[0m                  verbose_eval, metric_period, silent, early_stopping_rounds,\n\u001b[0;32m   5733\u001b[0m                  save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "File \u001b[1;32mc:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\catboost\\core.py:2355\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2351\u001b[0m allow_clear_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mallow_clear_pool\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   2353\u001b[0m \u001b[39mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[0;32m   2354\u001b[0m     plot_wrapper(plot, plot_file, \u001b[39m'\u001b[39m\u001b[39mTraining plots\u001b[39m\u001b[39m'\u001b[39m, [_get_train_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2355\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(\n\u001b[0;32m   2356\u001b[0m         train_pool,\n\u001b[0;32m   2357\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39meval_sets\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   2358\u001b[0m         params,\n\u001b[0;32m   2359\u001b[0m         allow_clear_pool,\n\u001b[0;32m   2360\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39minit_model\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m   2361\u001b[0m     )\n\u001b[0;32m   2363\u001b[0m \u001b[39m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2364\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_object\u001b[39m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32mc:\\Users\\levit\\.pyenv\\pyenv-win\\versions\\3.10.7\\lib\\site-packages\\catboost\\core.py:1759\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1758\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train\u001b[39m(\u001b[39mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1759\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_object\u001b[39m.\u001b[39;49m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[39m.\u001b[39;49m_object \u001b[39mif\u001b[39;49;00m init_model \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m   1760\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:4622\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:4671\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.015,\n",
       " 'depth': 5,\n",
       " 'l2_leaf_reg': 5.0,\n",
       " 'min_child_samples': 4}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = study.best_trial.params\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit ('3.10.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e748b08df6639b92156e1f0a2e584fc605f942beb5319c4ded409ee9197cfce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
