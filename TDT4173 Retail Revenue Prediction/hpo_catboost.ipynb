{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import *\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sqlalchemy import column\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#!pip install optuna \n",
    "import optuna\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_category(X):\n",
    "    for col in X.columns:\n",
    "        if ( (X[col].dtype != 'int64') and (X[col].dtype != 'float64') and (X[col].dtype != 'bool')):\n",
    "            X[col] = X[col].astype('category')\n",
    "            \n",
    "    return X\n",
    "\n",
    "def remove_outliers(df, column_names, n):\n",
    "    for column_name in column_names:\n",
    "        mean = df[column_name].mean()\n",
    "        std = df[column_name].std()\n",
    "        df = df[(df[column_name] > mean - n*std) & (df[column_name] < mean + n*std)]\n",
    "    return df\n",
    "    \n",
    "def log_func(y, shift_var):\n",
    "    return np.log(y+shift_var)\n",
    "\n",
    "def exp_func(y, shift_var):\n",
    "    return np.exp(y)-shift_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Root Mean Squared Logarithmic Error \n",
    "    \n",
    "    Args:\n",
    "        y_true (np.array): n-dimensional vector of ground-truth values \n",
    "        y_pred (np.array): n-dimensional vecotr of predicted values \n",
    "    \n",
    "    Returns:\n",
    "        A scalar float with the rmsle value \n",
    "    \n",
    "    Note: You can alternatively use sklearn and just do: \n",
    "        `sklearn.metrics.mean_squared_log_error(y_true, y_pred) ** 0.5`\n",
    "    \"\"\"\n",
    "    assert (y_true >= 0).all(), 'Received negative y_true values'\n",
    "    assert (y_pred >= 0).all(), 'Received negative y_pred values'\n",
    "    assert y_true.shape == y_pred.shape, 'y_true and y_pred have different shapes'\n",
    "    y_true_log1p = np.log1p(y_true)  # log(1 + y_true)\n",
    "    y_pred_log1p = np.log1p(y_pred)  # log(1 + y_pred)\n",
    "    return np.sqrt(np.mean(np.square(y_pred_log1p - y_true_log1p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = pd.read_csv('data/stores_train_with_extra_features.csv')\n",
    "stores_test = pd.read_csv('data/stores_test_with_extra_features.csv')\n",
    "\n",
    "plaace_hierarchy = pd.read_csv('data/plaace_hierarchy.csv')\n",
    "grunnkrets = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "grunnkrets_ages = pd.read_csv('data/grunnkrets_age_distribution.csv')\n",
    "grunnkrets_household_types = pd.read_csv('data/grunnkrets_households_num_persons.csv')\n",
    "grunnkrets_household_income = pd.read_csv('data/grunnkrets_income_households.csv')\n",
    "\n",
    "stores_train_copy= stores_train.copy()\n",
    "\n",
    "plaace_hierarchy_copy = plaace_hierarchy.copy()\n",
    "plaace_hierarchy_copy.drop(columns='sales_channel_name', inplace=True)\n",
    "plaace_hierarchy_copy['lv1'] = plaace_hierarchy_copy['lv1'].astype('category')\n",
    "plaace_hierarchy_copy['lv2'] = plaace_hierarchy_copy['lv2'].astype('category')\n",
    "\n",
    "\n",
    "grunnkrets_copy = grunnkrets.copy()\n",
    "grunnkrets_copy.rename(columns={'year': 'year_1'}, inplace=True)\n",
    "\n",
    "grunnkrets_ages_copy = grunnkrets_ages.copy()\n",
    "grunnkrets_ages_copy.rename(columns={'year': 'year_2'}, inplace=True)\n",
    "grunnkrets_ages_copy['grunnkrets_population'] = grunnkrets_ages_copy.iloc[:, 2:].sum(axis=1)\n",
    "\n",
    "grunnkrets_household_types_copy = grunnkrets_household_types.copy()\n",
    "grunnkrets_household_types_copy.rename(columns={'year': 'year_3'}, inplace=True)\n",
    "grunnkrets_household_types_copy[grunnkrets_household_types_copy.columns[2:]] = grunnkrets_household_types_copy[grunnkrets_household_types_copy.columns[2:]].astype('int64')\n",
    "\n",
    "grunnkrets_household_income_copy = grunnkrets_household_income.copy()\n",
    "grunnkrets_household_income_copy.rename(columns={'year': 'year_4', 'singles': 'singles_income','couple_without_children':'couple_without_children_income'}, inplace=True)\n",
    "grunnkrets_household_income_copy.rename(columns={'singles': 'singles_income', }, inplace=True)\n",
    "\n",
    "#set the values that are 0 to the lowest value in the column\n",
    "# for column in grunnkrets_household_income_copy.columns[2:]:\n",
    "#     grunnkrets_household_income_copy[column] = grunnkrets_household_income_copy[column].apply(lambda x: grunnkrets_household_income_copy[column].min() if x == 0 else x)\n",
    "    \n",
    "\n",
    "df = stores_train_copy\n",
    "\n",
    "df = pd.merge(df, plaace_hierarchy_copy, on='plaace_hierarchy_id', how='left')\n",
    "df = pd.merge(df, grunnkrets_copy, on='grunnkrets_id', how='left')\n",
    "df = pd.merge(df, grunnkrets_ages_copy, on='grunnkrets_id', how='left')\n",
    "df = pd.merge(df, grunnkrets_household_types_copy, on='grunnkrets_id', how='left')\n",
    "df = pd.merge(df, grunnkrets_household_income_copy, on='grunnkrets_id', how='left')\n",
    "\n",
    "\n",
    "\n",
    "df['grunnkrets_population_density'] = df['grunnkrets_population'] / df['area_km2']\n",
    "\n",
    "\n",
    "\n",
    "df.drop_duplicates(subset=['store_id'], keep='first', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "columns_to_drop = ['revenue',\n",
    "                  'store_id',\n",
    "                  'plaace_hierarchy_id',\n",
    "                  'grunnkrets_id',\n",
    "                  'year',\n",
    "                   'address',\n",
    "                  'store_name',\n",
    "                  \n",
    "                  'year_1',\n",
    "                  'geometry',\n",
    "                  \n",
    "                  #'area_km2',\n",
    "               \n",
    "                  'grunnkrets_name',\n",
    "                  'district_name',\n",
    "             \n",
    "                  'municipality_name',\n",
    "                  \n",
    "                  \n",
    "                  'year_2',\n",
    "                  'year_3',\n",
    "                  'year_4',\n",
    "                 \n",
    "                \n",
    "                  'sales_channel_name',\n",
    "                   #'mall_name',\n",
    "                  #'chain_name',\n",
    "                  \n",
    "                  \n",
    "                  \n",
    "                     'lv1',\n",
    "                  'lv2',\n",
    "                  'lv3',\n",
    "                  'lv4',\n",
    "                  \n",
    "                  # 'lv1_desc',\n",
    "                  # 'lv2_desc',\n",
    "                  # 'lv3_desc',\n",
    "                  # 'lv4_desc'\n",
    "                \n",
    "#                 'closest_busstop',\n",
    "                  \n",
    "#     'num_closest_busstops_250m',\n",
    "#    'num_closest_busstops_500m',\n",
    "\n",
    "#        'num_closest_busstops_1000m', 'num_closest_busstops_2500m',\n",
    "#        'num_closest_busstops_5000m', \n",
    "#        'num_closest_busstops',\n",
    "#        'num_closest_busstops_10000m', 'num_closest_busstops_15000m',\n",
    "       \n",
    "#        'closest_store_lv1', 'closest_store_lv2',\n",
    "#        'closest_store_lv3', 'closest_store_lv4', \n",
    "       \n",
    "#        'num_closest_stores_lv1_250m',   \n",
    "#    'num_closest_stores_lv1_500m', \n",
    "   \n",
    "#    'num_closest_stores_lv1_1000m',\n",
    "    \n",
    "#     'num_closest_stores_lv1_2500m',\n",
    "#        'num_closest_stores_lv1_5000m', \n",
    "#        'num_closest_stores_lv1',\n",
    "#        'num_closest_stores_lv1_10000m', \n",
    "# 'num_closest_stores_lv1_15000m',\n",
    "       \n",
    "#        'num_closest_stores_lv2_250m', \n",
    "   \n",
    "   \n",
    "#    'num_closest_stores_lv2_500m',\n",
    "   \n",
    "#     'num_closest_stores_lv2_1000m', \n",
    "    \n",
    "#     'num_closest_stores_lv2_2500m',\n",
    "#        'num_closest_stores_lv2_5000m',\n",
    "#        'num_closest_stores_lv2',\n",
    "#        'num_closest_stores_lv2_10000m', 'num_closest_stores_lv2_15000m',\n",
    "       \n",
    "#         'num_closest_stores_lv3_250m', \n",
    "   \n",
    "   \n",
    "#    'num_closest_stores_lv3_500m',\n",
    "   \n",
    "#        'num_closest_stores_lv3_1000m', 'num_closest_stores_lv3_2500m',\n",
    "#        'num_closest_stores_lv3_5000m', \n",
    "#        'num_closest_stores_lv3',\n",
    "#        'num_closest_stores_lv3_10000m', 'num_closest_stores_lv3_15000m',\n",
    "       \n",
    "#         'num_closest_stores_lv4_250m', \n",
    "   \n",
    "#    'num_closest_stores_lv4_500m',\n",
    "   \n",
    "#        'num_closest_stores_lv4_1000m', 'num_closest_stores_lv4_2500m',\n",
    "#        'num_closest_stores_lv4_5000m', \n",
    "#        'num_closest_stores_lv4',\n",
    "#        'num_closest_stores_lv4_10000m', 'num_closest_stores_lv4_15000m',\n",
    "#                   'distance_to_oslo',\n",
    "                  'distance_to_bergen',\n",
    "                  'distance_to_trondheim',\n",
    "                  'distance_to_stavanger',\n",
    "                  'distance_to_drammen',\n",
    "                  # 'grunnkrets_population',\n",
    "                  # 'grunnkrets_population_density',\n",
    "                  \n",
    "                  # 'num_closest_busstops_100m',\n",
    "                  # 'num_closest_busstops_750m',\n",
    "                  # 'num_closest_stores_lv1_100m',\n",
    "                  # 'num_closest_stores_lv1_750m', \n",
    "                  # 'num_closest_stores_lv2_100m',\n",
    "                  # 'num_closest_stores_lv2_750m',\n",
    "                  # 'num_closest_stores_lv3_100m',\n",
    "                  # 'num_closest_stores_lv3_750m',\n",
    "              \n",
    "                  # 'num_closest_stores_lv4_750m',\n",
    "                  \n",
    "                  ]\n",
    "\n",
    "\n",
    "# fill in missing values of float columns with mean\n",
    "\n",
    "\n",
    "X = df.drop(columns=columns_to_drop)\n",
    "X = convert_to_category(X)\n",
    "\n",
    "y = df.revenue\n",
    "y = np.log1p(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "categorical_features = X.select_dtypes(include=['category']).columns\n",
    "numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "\n",
    "# One hot encoding\n",
    "        \n",
    "full_pipeline = ColumnTransformer([('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)], remainder='passthrough')\n",
    "one_hot_encoder = full_pipeline.fit(X)\n",
    "X_encoded_one_hot = one_hot_encoder.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 12\n",
    "X_selected = X_encoded_one_hot.copy()\n",
    "\n",
    "def objective(trial,data=X_encoded_one_hot,target=y):\n",
    " \n",
    "    param = {}\n",
    "    param['learning_rate'] = trial.suggest_discrete_uniform(\"learning_rate\", 0.001, 0.1, 0.001)\n",
    "    param['depth'] = trial.suggest_int('depth', 2, 10)\n",
    "    param['l2_leaf_reg'] = trial.suggest_discrete_uniform('l2_leaf_reg', 1.0, 5.5, 0.5)\n",
    "    param['min_child_samples'] = trial.suggest_categorical('min_child_samples', [1, 4, 8, 16, 32])\n",
    "    #param['grow_policy'] = 'Depthwise'\n",
    "    param['iterations'] = trial.suggest_int('iterations', 10, 3000)\n",
    "    #param['use_best_model'] = True\n",
    "    param['eval_metric'] = 'RMSE'\n",
    "    #param['od_type'] = 'iter'\n",
    "    # param['od_wait'] = 20\n",
    "    # param['random_state'] = 42\n",
    "    # param['logging_level'] = 'Silent'\n",
    "    param['task_type'] = 'GPU'\n",
    "    \n",
    "    # params = {\n",
    "    #     #'iterations':trial.suggest_int(\"iterations\", 4000, 25000),\n",
    "    #     #'od_wait':trial.suggest_int('od_wait', 500, 2300),\n",
    "    #     'learning_rate' : trial.suggest_uniform('learning_rate',0.01, 1),\n",
    "    #     'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n",
    "    #     'subsample': trial.suggest_uniform('subsample',0,1),\n",
    "    #     'random_strength': trial.suggest_uniform('random_strength',10,50),\n",
    "    #     'depth': trial.suggest_int('depth',1, 9),\n",
    "    #     'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n",
    "    #     'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n",
    "    #     'loss_function':\"RMSE\",\n",
    "    # }\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = cat.CatBoostRegressor(**param)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    kf_scores = np.empty(5)\n",
    "    \n",
    "    X_selected = X_encoded_one_hot\n",
    "    \n",
    "    for idx, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
    "        X_train, X_test = X_selected[train_index], X_selected[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "  \n",
    "        model.fit(X_train, y_train,eval_set=[(X_test,y_test)], early_stopping_rounds=100, verbose=False)\n",
    "    \n",
    "        preds = model.predict(X_test)\n",
    "        \n",
    "        y_test_exp = np.expm1(y_test)\n",
    "        preds_exp = np.expm1(preds)\n",
    "        \n",
    "        preds_exp = np.where(preds_exp < 0, 0, preds_exp)\n",
    "        \n",
    "        kf_scores[idx] = rmsle(y_test_exp, preds_exp)\n",
    "\n",
    "    return np.mean(kf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-10 07:34:01,698]\u001b[0m A new study created in memory with name: no-name-27946f3d-e40a-417f-94d0-f0c561cf0eed\u001b[0m\n",
      "\u001b[32m[I 2022-11-10 07:34:22,907]\u001b[0m Trial 0 finished with value: 0.7336654791359674 and parameters: {'learning_rate': 0.092, 'depth': 9, 'l2_leaf_reg': 1.5, 'min_child_samples': 1, 'iterations': 200}. Best is trial 0 with value: 0.7336654791359674.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.066,\n",
       " 'depth': 8,\n",
       " 'l2_leaf_reg': 2.5,\n",
       " 'min_child_samples': 8,\n",
       " 'iterations': 800}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = study.best_trial.params\n",
    "val = study.best_trial.value\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "params = study.best_trial.params\n",
    "val = study.best_trial.value\n",
    "\n",
    "dict = {'val': val, 'params': params}\n",
    "\n",
    "# save the params and value\n",
    "\n",
    "with open('cat_params.json', 'w') as fp:\n",
    "    json.dump(dict, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit ('3.10.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e748b08df6639b92156e1f0a2e584fc605f942beb5319c4ded409ee9197cfce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
